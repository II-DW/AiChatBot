{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    ['한국어', '자연어', '처리를', '쉽게', '시작해봅시다'],\n",
    "    ['KoNLPy를', '사용하여', '한국어', '텍스트를', '처리합니다']\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Index: {'한국어': 1, '자연어': 2, '처리를': 3, '쉽게': 4, '시작해봅시다': 5, 'konlpy를': 6, '사용하여': 7, '텍스트를': 8, '처리합니다': 9}\n",
      "Sequences: [[1, 2, 3, 4, 5], [6, 7, 1, 8, 9]]\n",
      "One-hot encoded data:\n",
      "[[[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "  [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "\n",
    "# Get the word index (index assigned to each morpheme)\n",
    "word_index = tokenizer.word_index\n",
    "print(\"Word Index:\", word_index)\n",
    "\n",
    "# Convert sentences to sequences of indices\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "print(\"Sequences:\", sequences)\n",
    "\n",
    "# One-hot encode the sequences\n",
    "one_hot_encoded = to_categorical(sequences)\n",
    "print(\"One-hot encoded data:\")\n",
    "print(one_hot_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>intent</th>\n",
       "      <th>sentence</th>\n",
       "      <th>QorA</th>\n",
       "      <th>NLP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>인사</td>\n",
       "      <td>안녕하쇼</td>\n",
       "      <td>Q</td>\n",
       "      <td>[안녕, 하, 쇼]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>인사</td>\n",
       "      <td>안녕</td>\n",
       "      <td>Q</td>\n",
       "      <td>[안녕]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>인사</td>\n",
       "      <td>안녕하세요</td>\n",
       "      <td>Q</td>\n",
       "      <td>[안녕하세요]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>인사</td>\n",
       "      <td>ㅎㅇ</td>\n",
       "      <td>Q</td>\n",
       "      <td>[ㅎㅇ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>인사</td>\n",
       "      <td>ㅎㅇㅇ</td>\n",
       "      <td>Q</td>\n",
       "      <td>[ㅎㅇㅇ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>대건고등학교의 교화</td>\n",
       "      <td>대건고 꽃</td>\n",
       "      <td>Q</td>\n",
       "      <td>[대, 건고, 꽃]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>대건고등학교의 교화</td>\n",
       "      <td>대건고의 꽃은?</td>\n",
       "      <td>Q</td>\n",
       "      <td>[대, 건고, 의, 꽃, 은, ?]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>대건고등학교의 교화</td>\n",
       "      <td>대건고등학교의 꽃은?</td>\n",
       "      <td>Q</td>\n",
       "      <td>[대건고등학교, 의, 꽃, 은, ?]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>대건고등학교의 교화</td>\n",
       "      <td>대건고등학교의 꽃은 무엇이니?</td>\n",
       "      <td>Q</td>\n",
       "      <td>[대건고등학교, 의, 꽃, 은, 무엇, 이, 니, ?]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>대건고등학교의 교화</td>\n",
       "      <td>대건고등학교의 꽃은 무엇이니</td>\n",
       "      <td>Q</td>\n",
       "      <td>[대건고등학교, 의, 꽃, 은, 무엇, 이, 니]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>84 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        intent          sentence QorA                             NLP\n",
       "0           인사              안녕하쇼    Q                      [안녕, 하, 쇼]\n",
       "1           인사                안녕    Q                            [안녕]\n",
       "2           인사             안녕하세요    Q                         [안녕하세요]\n",
       "3           인사                ㅎㅇ    Q                            [ㅎㅇ]\n",
       "4           인사               ㅎㅇㅇ    Q                           [ㅎㅇㅇ]\n",
       "..         ...               ...  ...                             ...\n",
       "79  대건고등학교의 교화             대건고 꽃    Q                      [대, 건고, 꽃]\n",
       "80  대건고등학교의 교화          대건고의 꽃은?    Q             [대, 건고, 의, 꽃, 은, ?]\n",
       "81  대건고등학교의 교화       대건고등학교의 꽃은?    Q            [대건고등학교, 의, 꽃, 은, ?]\n",
       "82  대건고등학교의 교화  대건고등학교의 꽃은 무엇이니?    Q  [대건고등학교, 의, 꽃, 은, 무엇, 이, 니, ?]\n",
       "83  대건고등학교의 교화   대건고등학교의 꽃은 무엇이니    Q     [대건고등학교, 의, 꽃, 은, 무엇, 이, 니]\n",
       "\n",
       "[84 rows x 4 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_pickle('NLP_df.pkl')\n",
    "df = df[df['QorA'] == 'Q']\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create_Word_Index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['안녕', '하', '쇼'],\n",
       " ['안녕'],\n",
       " ['안녕하세요'],\n",
       " ['ㅎㅇ'],\n",
       " ['ㅎㅇㅇ'],\n",
       " ['방', '가', '링'],\n",
       " ['방', '가방', '가'],\n",
       " ['헬로우'],\n",
       " ['하이'],\n",
       " ['잘', '있', '어'],\n",
       " ['다음', '에', '보', '자'],\n",
       " ['잘', '가', '아'],\n",
       " ['나', '갈', '게'],\n",
       " ['좋은 하루', '보내', '어'],\n",
       " ['ㅂㅂ'],\n",
       " ['바이바이'],\n",
       " ['ㅂㅇ'],\n",
       " ['대건고등학교', '는', '어디', '에', '있', '니', '?'],\n",
       " ['대건고등학교', '는', '어디', '에', '위치', '하', '니', '?'],\n",
       " ['대건고등학교', '는', '어디', '에', '위치', '하', '아', '?'],\n",
       " ['대건고등학교', '는', '어디', '에', '있', '어', '?'],\n",
       " ['대건고등학교', '위치'],\n",
       " ['대건고등학교', '의', '위치'],\n",
       " ['대', '건고', '위치'],\n",
       " ['대', '건고', '의', '위치'],\n",
       " ['대', '건고', '는', '어디', '에', '위치', '하', '니', '?'],\n",
       " ['대', '건고', '는', '어디', '에', '위치', '하', '아', '있', '어', '?'],\n",
       " ['대', '건고', '의', '위치', '는', '?'],\n",
       " ['대건고등학교', '의', '교훈', '이', '뭐', '야', '?'],\n",
       " ['대건고등학교', '교훈'],\n",
       " ['대건고등학교', '는', '교훈', '이', '뭐', '야', '?'],\n",
       " ['대건고등학교', '의', '교훈', '은', '?'],\n",
       " ['대', '건고', '교훈'],\n",
       " ['대', '건고', '는', '교훈', '이', '뭐', '야', '?'],\n",
       " ['대', '건고', '의', '교훈', '이', '뭐', '야', '?'],\n",
       " ['대', '건고', '의', '교훈', '은', '?'],\n",
       " ['대건고등학교', '의', '교목', '이', '뭐', '야', '?'],\n",
       " ['대건고등학교', '의', '교목'],\n",
       " ['대건고등학교', '의', '교목'],\n",
       " ['대', '건고', '교목'],\n",
       " ['대', '건고', '의', '교목', '이', '뭐', '야', '?'],\n",
       " ['대', '건고', '의', '교목', '은', '?'],\n",
       " ['대', '건고', '는', '교목', '이', '뭐', '야', '?'],\n",
       " ['대건고등학교', '는', '교목', '이', '뭐', '야', '?'],\n",
       " ['대건', '교의', '교목', '은', '?'],\n",
       " ['대건고등학교', '의', '교목', '은', '?'],\n",
       " ['대건고등학교', '의', '교목', '은', '무엇', '이', '니', '?'],\n",
       " ['대', '건고', '의', '교목', '은', '무엇', '이', '니', '?'],\n",
       " ['대건고등학교', '의', '교목', '이', '뭐', 'ㄹ까', '?'],\n",
       " ['대', '건고', '의', '교목', '이', '뭐', 'ㄹ까'],\n",
       " ['대', '건고', '의', '교목', '은', '무엇', '이', '니'],\n",
       " ['대', '건고', '의', '교목', '은', '무엇', '이', 'ㄹ까'],\n",
       " ['대건고등학교', '의', '교목', '은', '무엇', '이', '니'],\n",
       " ['대건고등학교', '의', '교목', '은', '무엇', '이', 'ㄹ까'],\n",
       " ['대', '건고', '의', '나무', '는', '무엇', '이', 'ㄹ까'],\n",
       " ['대', '건고', '나무'],\n",
       " ['대', '건고', '의', '나무', '는', '?'],\n",
       " ['대건고등학교', '의', '나무', '는', '?'],\n",
       " ['대건고등학교', '의', '나무', '는', '무엇', '이', '니', '?'],\n",
       " ['대건고등학교', '의', '나무', '는', '무엇', '이', '니'],\n",
       " ['대건고등학교', '의', '교화', '는', '뭐', '야', '?'],\n",
       " ['대건고등학교', '의', '교화'],\n",
       " ['대건고등학교', '의', '교화'],\n",
       " ['대', '건고', '교화'],\n",
       " ['대', '건고', '의', '교화', '가', '뭐', '야', '?'],\n",
       " ['대', '건고', '의', '교화', '는', '?'],\n",
       " ['대', '건고', '는', '교화', '이', '뭐', '야', '?'],\n",
       " ['대건고등학교', '는', '교화', '가', '뭐', '야', '?'],\n",
       " ['대건', '교의', '교화', '는', '?'],\n",
       " ['대건고등학교', '의', '교화', '는', '?'],\n",
       " ['대건고등학교', '의', '교화', '는', '무엇', '이', '니', '?'],\n",
       " ['대', '건고', '의', '교화', '는', '무엇', '이', '니', '?'],\n",
       " ['대건고등학교', '의', '교화', '는', '뭐', 'ㄹ까', '?'],\n",
       " ['대', '건고', '의', '교화', '가', '뭐', 'ㄹ까'],\n",
       " ['대', '건고', '의', '교화', '는', '무엇', '이', '니'],\n",
       " ['대', '건고', '의', '교화', '는', '무엇', '이', 'ㄹ까'],\n",
       " ['대건고등학교', '의', '교화', '는', '무엇', '이', '니'],\n",
       " ['대건고등학교', '의', '교화', '는', '무엇', '이', 'ㄹ까'],\n",
       " ['대', '건고', '의', '꽃', '은', '무엇', '이', 'ㄹ까'],\n",
       " ['대', '건고', '꽃'],\n",
       " ['대', '건고', '의', '꽃', '은', '?'],\n",
       " ['대건고등학교', '의', '꽃', '은', '?'],\n",
       " ['대건고등학교', '의', '꽃', '은', '무엇', '이', '니', '?'],\n",
       " ['대건고등학교', '의', '꽃', '은', '무엇', '이', '니']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = []\n",
    "for s in df['NLP'].iloc[:] :\n",
    "    data.append(s)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Tokenizer and fit on the sentences\n",
    "tokenizer = Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Index: {'의': 1, '?': 2, '대건고등학교': 3, '대': 4, '건고': 5, '는': 6, '이': 7, '교목': 8, '무엇': 9, '교화': 10, '뭐': 11, '은': 12, '니': 13, '야': 14, 'ㄹ까': 15, '위치': 16, '교훈': 17, '에': 18, '가': 19, '어디': 20, '나무': 21, '꽃': 22, '하': 23, '있': 24, '어': 25, '아': 26, '안녕': 27, '방': 28, '잘': 29, '대건': 30, '교의': 31, '쇼': 32, '안녕하세요': 33, 'ㅎㅇ': 34, 'ㅎㅇㅇ': 35, '링': 36, '가방': 37, '헬로우': 38, '하이': 39, '다음': 40, '보': 41, '자': 42, '나': 43, '갈': 44, '게': 45, '좋은 하루': 46, '보내': 47, 'ㅂㅂ': 48, '바이바이': 49, 'ㅂㅇ': 50}\n"
     ]
    }
   ],
   "source": [
    "# Get the word index (index assigned to each morpheme)\n",
    "word_index = tokenizer.word_index\n",
    "print(\"Word Index:\", word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequences: [[27, 23, 32], [27], [33], [34], [35], [28, 19, 36], [28, 37, 19], [38], [39], [29, 24, 25], [40, 18, 41, 42], [29, 19, 26], [43, 44, 45], [46, 47, 25], [48], [49], [50], [3, 6, 20, 18, 24, 13, 2], [3, 6, 20, 18, 16, 23, 13, 2], [3, 6, 20, 18, 16, 23, 26, 2], [3, 6, 20, 18, 24, 25, 2], [3, 16], [3, 1, 16], [4, 5, 16], [4, 5, 1, 16], [4, 5, 6, 20, 18, 16, 23, 13, 2], [4, 5, 6, 20, 18, 16, 23, 26, 24, 25, 2], [4, 5, 1, 16, 6, 2], [3, 1, 17, 7, 11, 14, 2], [3, 17], [3, 6, 17, 7, 11, 14, 2], [3, 1, 17, 12, 2], [4, 5, 17], [4, 5, 6, 17, 7, 11, 14, 2], [4, 5, 1, 17, 7, 11, 14, 2], [4, 5, 1, 17, 12, 2], [3, 1, 8, 7, 11, 14, 2], [3, 1, 8], [3, 1, 8], [4, 5, 8], [4, 5, 1, 8, 7, 11, 14, 2], [4, 5, 1, 8, 12, 2], [4, 5, 6, 8, 7, 11, 14, 2], [3, 6, 8, 7, 11, 14, 2], [30, 31, 8, 12, 2], [3, 1, 8, 12, 2], [3, 1, 8, 12, 9, 7, 13, 2], [4, 5, 1, 8, 12, 9, 7, 13, 2], [3, 1, 8, 7, 11, 15, 2], [4, 5, 1, 8, 7, 11, 15], [4, 5, 1, 8, 12, 9, 7, 13], [4, 5, 1, 8, 12, 9, 7, 15], [3, 1, 8, 12, 9, 7, 13], [3, 1, 8, 12, 9, 7, 15], [4, 5, 1, 21, 6, 9, 7, 15], [4, 5, 21], [4, 5, 1, 21, 6, 2], [3, 1, 21, 6, 2], [3, 1, 21, 6, 9, 7, 13, 2], [3, 1, 21, 6, 9, 7, 13], [3, 1, 10, 6, 11, 14, 2], [3, 1, 10], [3, 1, 10], [4, 5, 10], [4, 5, 1, 10, 19, 11, 14, 2], [4, 5, 1, 10, 6, 2], [4, 5, 6, 10, 7, 11, 14, 2], [3, 6, 10, 19, 11, 14, 2], [30, 31, 10, 6, 2], [3, 1, 10, 6, 2], [3, 1, 10, 6, 9, 7, 13, 2], [4, 5, 1, 10, 6, 9, 7, 13, 2], [3, 1, 10, 6, 11, 15, 2], [4, 5, 1, 10, 19, 11, 15], [4, 5, 1, 10, 6, 9, 7, 13], [4, 5, 1, 10, 6, 9, 7, 15], [3, 1, 10, 6, 9, 7, 13], [3, 1, 10, 6, 9, 7, 15], [4, 5, 1, 22, 12, 9, 7, 15], [4, 5, 22], [4, 5, 1, 22, 12, 2], [3, 1, 22, 12, 2], [3, 1, 22, 12, 9, 7, 13, 2], [3, 1, 22, 12, 9, 7, 13]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Convert sentences to sequences of indices\n",
    "sequences = tokenizer.texts_to_sequences(data)\n",
    "print(\"Sequences:\", sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padded Sequences: [[27 23 32  0  0  0  0  0  0  0]\n",
      " [27  0  0  0  0  0  0  0  0  0]\n",
      " [33  0  0  0  0  0  0  0  0  0]\n",
      " [34  0  0  0  0  0  0  0  0  0]\n",
      " [35  0  0  0  0  0  0  0  0  0]\n",
      " [28 19 36  0  0  0  0  0  0  0]\n",
      " [28 37 19  0  0  0  0  0  0  0]\n",
      " [38  0  0  0  0  0  0  0  0  0]\n",
      " [39  0  0  0  0  0  0  0  0  0]\n",
      " [29 24 25  0  0  0  0  0  0  0]\n",
      " [40 18 41 42  0  0  0  0  0  0]\n",
      " [29 19 26  0  0  0  0  0  0  0]\n",
      " [43 44 45  0  0  0  0  0  0  0]\n",
      " [46 47 25  0  0  0  0  0  0  0]\n",
      " [48  0  0  0  0  0  0  0  0  0]\n",
      " [49  0  0  0  0  0  0  0  0  0]\n",
      " [50  0  0  0  0  0  0  0  0  0]\n",
      " [ 3  6 20 18 24 13  2  0  0  0]\n",
      " [ 3  6 20 18 16 23 13  2  0  0]\n",
      " [ 3  6 20 18 16 23 26  2  0  0]\n",
      " [ 3  6 20 18 24 25  2  0  0  0]\n",
      " [ 3 16  0  0  0  0  0  0  0  0]\n",
      " [ 3  1 16  0  0  0  0  0  0  0]\n",
      " [ 4  5 16  0  0  0  0  0  0  0]\n",
      " [ 4  5  1 16  0  0  0  0  0  0]\n",
      " [ 4  5  6 20 18 16 23 13  2  0]\n",
      " [ 5  6 20 18 16 23 26 24 25  2]\n",
      " [ 4  5  1 16  6  2  0  0  0  0]\n",
      " [ 3  1 17  7 11 14  2  0  0  0]\n",
      " [ 3 17  0  0  0  0  0  0  0  0]\n",
      " [ 3  6 17  7 11 14  2  0  0  0]\n",
      " [ 3  1 17 12  2  0  0  0  0  0]\n",
      " [ 4  5 17  0  0  0  0  0  0  0]\n",
      " [ 4  5  6 17  7 11 14  2  0  0]\n",
      " [ 4  5  1 17  7 11 14  2  0  0]\n",
      " [ 4  5  1 17 12  2  0  0  0  0]\n",
      " [ 3  1  8  7 11 14  2  0  0  0]\n",
      " [ 3  1  8  0  0  0  0  0  0  0]\n",
      " [ 3  1  8  0  0  0  0  0  0  0]\n",
      " [ 4  5  8  0  0  0  0  0  0  0]\n",
      " [ 4  5  1  8  7 11 14  2  0  0]\n",
      " [ 4  5  1  8 12  2  0  0  0  0]\n",
      " [ 4  5  6  8  7 11 14  2  0  0]\n",
      " [ 3  6  8  7 11 14  2  0  0  0]\n",
      " [30 31  8 12  2  0  0  0  0  0]\n",
      " [ 3  1  8 12  2  0  0  0  0  0]\n",
      " [ 3  1  8 12  9  7 13  2  0  0]\n",
      " [ 4  5  1  8 12  9  7 13  2  0]\n",
      " [ 3  1  8  7 11 15  2  0  0  0]\n",
      " [ 4  5  1  8  7 11 15  0  0  0]\n",
      " [ 4  5  1  8 12  9  7 13  0  0]\n",
      " [ 4  5  1  8 12  9  7 15  0  0]\n",
      " [ 3  1  8 12  9  7 13  0  0  0]\n",
      " [ 3  1  8 12  9  7 15  0  0  0]\n",
      " [ 4  5  1 21  6  9  7 15  0  0]\n",
      " [ 4  5 21  0  0  0  0  0  0  0]\n",
      " [ 4  5  1 21  6  2  0  0  0  0]\n",
      " [ 3  1 21  6  2  0  0  0  0  0]\n",
      " [ 3  1 21  6  9  7 13  2  0  0]\n",
      " [ 3  1 21  6  9  7 13  0  0  0]\n",
      " [ 3  1 10  6 11 14  2  0  0  0]\n",
      " [ 3  1 10  0  0  0  0  0  0  0]\n",
      " [ 3  1 10  0  0  0  0  0  0  0]\n",
      " [ 4  5 10  0  0  0  0  0  0  0]\n",
      " [ 4  5  1 10 19 11 14  2  0  0]\n",
      " [ 4  5  1 10  6  2  0  0  0  0]\n",
      " [ 4  5  6 10  7 11 14  2  0  0]\n",
      " [ 3  6 10 19 11 14  2  0  0  0]\n",
      " [30 31 10  6  2  0  0  0  0  0]\n",
      " [ 3  1 10  6  2  0  0  0  0  0]\n",
      " [ 3  1 10  6  9  7 13  2  0  0]\n",
      " [ 4  5  1 10  6  9  7 13  2  0]\n",
      " [ 3  1 10  6 11 15  2  0  0  0]\n",
      " [ 4  5  1 10 19 11 15  0  0  0]\n",
      " [ 4  5  1 10  6  9  7 13  0  0]\n",
      " [ 4  5  1 10  6  9  7 15  0  0]\n",
      " [ 3  1 10  6  9  7 13  0  0  0]\n",
      " [ 3  1 10  6  9  7 15  0  0  0]\n",
      " [ 4  5  1 22 12  9  7 15  0  0]\n",
      " [ 4  5 22  0  0  0  0  0  0  0]\n",
      " [ 4  5  1 22 12  2  0  0  0  0]\n",
      " [ 3  1 22 12  2  0  0  0  0  0]\n",
      " [ 3  1 22 12  9  7 13  2  0  0]\n",
      " [ 3  1 22 12  9  7 13  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Pad sequences to the same length (optional step)\n",
    "max_sequence_length = 10\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length, padding='post')\n",
    "print(\"Padded Sequences:\", padded_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-hot encoded data:\n",
      "[[[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 1. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 1. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 1. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 1. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# One-hot encode the sequences\n",
    "one_hot_encoded = to_categorical(padded_sequences, num_classes=len(word_index) + 1)\n",
    "print(\"One-hot encoded data:\")\n",
    "print(one_hot_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('One-Hot.pkl','wb') as f:\n",
    "   pickle.dump(one_hot_encoded,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Word_Dict.pkl','wb') as f:\n",
    "   pickle.dump(word_index,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
