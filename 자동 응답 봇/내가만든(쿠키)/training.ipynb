{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Word_Dict.pkl\",\"rb\") as f:\n",
    "    word_index = pickle.load(f)\n",
    "\n",
    "with open(\"One-Hot.pkl\",\"rb\") as f:\n",
    "    One_Hot = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(84, 10, 51)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "One_Hot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>intent</th>\n",
       "      <th>sentence</th>\n",
       "      <th>QorA</th>\n",
       "      <th>NLP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>인사</td>\n",
       "      <td>안녕하쇼</td>\n",
       "      <td>Q</td>\n",
       "      <td>[안녕, 하, 쇼]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>인사</td>\n",
       "      <td>안녕</td>\n",
       "      <td>Q</td>\n",
       "      <td>[안녕]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>인사</td>\n",
       "      <td>안녕하세요</td>\n",
       "      <td>Q</td>\n",
       "      <td>[안녕하세요]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>인사</td>\n",
       "      <td>ㅎㅇ</td>\n",
       "      <td>Q</td>\n",
       "      <td>[ㅎㅇ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>인사</td>\n",
       "      <td>ㅎㅇㅇ</td>\n",
       "      <td>Q</td>\n",
       "      <td>[ㅎㅇㅇ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>대건고등학교의 교화</td>\n",
       "      <td>대건고 꽃</td>\n",
       "      <td>Q</td>\n",
       "      <td>[대, 건고, 꽃]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>대건고등학교의 교화</td>\n",
       "      <td>대건고의 꽃은?</td>\n",
       "      <td>Q</td>\n",
       "      <td>[대, 건고, 의, 꽃, 은, ?]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>대건고등학교의 교화</td>\n",
       "      <td>대건고등학교의 꽃은?</td>\n",
       "      <td>Q</td>\n",
       "      <td>[대건고등학교, 의, 꽃, 은, ?]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>대건고등학교의 교화</td>\n",
       "      <td>대건고등학교의 꽃은 무엇이니?</td>\n",
       "      <td>Q</td>\n",
       "      <td>[대건고등학교, 의, 꽃, 은, 무엇, 이, 니, ?]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>대건고등학교의 교화</td>\n",
       "      <td>대건고등학교의 꽃은 무엇이니</td>\n",
       "      <td>Q</td>\n",
       "      <td>[대건고등학교, 의, 꽃, 은, 무엇, 이, 니]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>84 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        intent          sentence QorA                             NLP\n",
       "0           인사              안녕하쇼    Q                      [안녕, 하, 쇼]\n",
       "1           인사                안녕    Q                            [안녕]\n",
       "2           인사             안녕하세요    Q                         [안녕하세요]\n",
       "3           인사                ㅎㅇ    Q                            [ㅎㅇ]\n",
       "4           인사               ㅎㅇㅇ    Q                           [ㅎㅇㅇ]\n",
       "..         ...               ...  ...                             ...\n",
       "79  대건고등학교의 교화             대건고 꽃    Q                      [대, 건고, 꽃]\n",
       "80  대건고등학교의 교화          대건고의 꽃은?    Q             [대, 건고, 의, 꽃, 은, ?]\n",
       "81  대건고등학교의 교화       대건고등학교의 꽃은?    Q            [대건고등학교, 의, 꽃, 은, ?]\n",
       "82  대건고등학교의 교화  대건고등학교의 꽃은 무엇이니?    Q  [대건고등학교, 의, 꽃, 은, 무엇, 이, 니, ?]\n",
       "83  대건고등학교의 교화   대건고등학교의 꽃은 무엇이니    Q     [대건고등학교, 의, 꽃, 은, 무엇, 이, 니]\n",
       "\n",
       "[84 rows x 4 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_pickle('NLP_df.pkl')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lstm_model(input_shape, num_classes):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(64, input_shape=input_shape))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming your input data shape is (num_samples, time_steps, num_features)\n",
    "input_shape = (10, 51)  # Adjust accordingly to your data\n",
    "\n",
    "vocab_size = len(word_index) # Size of the vocabulary (number of unique tokenized morphemes)\n",
    "embedding_dim = 200  # Dimension of word embeddings\n",
    "max_sequence_length = 10  # Maximum length of input sequences (after padding/truncating)\n",
    "num_classes = len(df['intent'].unique())  # Number of complaint categories (number of classes to predict)\n",
    "hidden_units = 64  # Number of LSTM units (hidden units in the LSTM layer)\n",
    "\n",
    "# Number of intent classes (6 in this case)\n",
    "num_classes = 6\n",
    "\n",
    "model = create_lstm_model(input_shape, num_classes)\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 1., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 1., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 1., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 1., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.]]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "One_Hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "       4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5,\n",
       "       5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = []\n",
    "n = 0\n",
    "for i in df['intent'].unique() :\n",
    "    for a in range(len(df[df['intent']==i])) :\n",
    "        y.append(n)\n",
    "    n += 1\n",
    "y = np.array(y)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(One_Hot, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 1s 327ms/step - loss: 1.7873 - accuracy: 0.3019 - val_loss: 1.7861 - val_accuracy: 0.2857\n",
      "Epoch 2/10\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 1.7679 - accuracy: 0.3019 - val_loss: 1.7775 - val_accuracy: 0.2857\n",
      "Epoch 3/10\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 1.7512 - accuracy: 0.3019 - val_loss: 1.7690 - val_accuracy: 0.2857\n",
      "Epoch 4/10\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 1.7359 - accuracy: 0.3019 - val_loss: 1.7624 - val_accuracy: 0.2857\n",
      "Epoch 5/10\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 1.7178 - accuracy: 0.3019 - val_loss: 1.7552 - val_accuracy: 0.2857\n",
      "Epoch 6/10\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 1.7059 - accuracy: 0.3019 - val_loss: 1.7488 - val_accuracy: 0.2857\n",
      "Epoch 7/10\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 1.6861 - accuracy: 0.3019 - val_loss: 1.7411 - val_accuracy: 0.2857\n",
      "Epoch 8/10\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 1.6694 - accuracy: 0.3019 - val_loss: 1.7326 - val_accuracy: 0.2857\n",
      "Epoch 9/10\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 1.6527 - accuracy: 0.3019 - val_loss: 1.7234 - val_accuracy: 0.2857\n",
      "Epoch 10/10\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 1.6345 - accuracy: 0.3019 - val_loss: 1.7129 - val_accuracy: 0.2857\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1289d59f400>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assuming your data is stored in X_train (input data) and y_train (target labels)\n",
    "model.fit(X_train, y_train, batch_size=32, epochs=10, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 20ms/step - loss: 1.6855 - accuracy: 0.2353\n",
      "Test loss: 1.685490369796753, Test accuracy: 0.23529411852359772\n"
     ]
    }
   ],
   "source": [
    "# Assuming your test data is stored in X_test and y_test\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Test loss: {loss}, Test accuracy: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Komoran\n",
    "komoran=Komoran()\n",
    "import json\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "#tokenizer.fit_on_texts(sentences)\n",
    "#word_index = tokenizer.word_index\n",
    "#sequences = tokenizer.texts_to_sequences(sentences)\n",
    "#one_hot_encoded = to_categorical(sequences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(word_index.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(complaint, word_index, max_sequence_length, tokenizer, df):\n",
    "    # Create a Tokenizer and fit on the sentences\n",
    "    tokenizer = Tokenizer()\n",
    "\n",
    "\n",
    "    data = []\n",
    "    for s in df['NLP'].iloc[:] :\n",
    "        data.append(s)\n",
    "\n",
    "    tokenizer.fit_on_texts(data)\n",
    "\n",
    "    tokenized_complaint = komoran.morphs(complaint)  # Implement your tokenization function\n",
    "\n",
    "    li = []\n",
    "\n",
    "    for i in tokenized_complaint :\n",
    "        if i in list(word_index.keys()) :\n",
    "            li.append(word_index[i])\n",
    "        else : \n",
    "            li.append(len(list(word_index.keys())) + 1)\n",
    "\n",
    "\n",
    "    max_sequence_length = 10\n",
    "    padded_sequences = pad_sequences([li], maxlen=max_sequence_length, padding='post')\n",
    "\n",
    "    print(padded_sequences)\n",
    "    one_hot_encoded = to_categorical(padded_sequences, num_classes=len(word_index) + 1)\n",
    "\n",
    "    return one_hot_encoded\n",
    "\n",
    "def map_index_to_category(predicted_index):\n",
    "    categories = {0: '인사', 1: '작별인사', 2: '대건고 위치',\n",
    "                  3: '대건고 교훈', 4: '대건고 교목', 5: '대건고 교화'}\n",
    "\n",
    "    return categories[predicted_index]\n",
    "\n",
    "def predict_category(model, complaint, word_index, max_sequence_length, tokenizer, df):\n",
    "    # Preprocess the complaint\n",
    "    X_new_data = preprocess_text(complaint, word_index, max_sequence_length, tokenizer, df)\n",
    "\n",
    "    # Make predictions using the model\n",
    "    predictions = model.predict(X_new_data)\n",
    "    \n",
    "    print(X_new_data)\n",
    "\n",
    "    # Get the index of the category with the highest probability\n",
    "    predicted_classes = predictions.argmax(axis=-1)\n",
    "\n",
    "    print(predicted_classes)\n",
    "    \n",
    "    category = map_index_to_category(predicted_classes[0])\n",
    "\n",
    "    return category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3  1  8  7 11 14  2  0  0  0]]\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "[[[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0.]\n",
      "  [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0.]\n",
      "  [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0.]\n",
      "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0.]\n",
      "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0.]\n",
      "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0.]]]\n",
      "[5]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'대건고 교화'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('Word_Dict.pkl','rb') as f:\n",
    "   word_index = pickle.load(f)\n",
    "\n",
    "user_complaint = \"대건고등학교의 교목이 뭐야?\"\n",
    "max_sequence_length = 10\n",
    "\n",
    "category = predict_category(model, user_complaint, word_index, max_sequence_length, tokenizer, df)\n",
    "category\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compiled_chatbot_model.save('Chatbot_Model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Load the trained model\n",
    "#model = tf.keras.models.load_model('Chatbot_Model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(67, 10, 51)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
